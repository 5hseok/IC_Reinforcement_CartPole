{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer  #KBinsDiscretizer를 사용하기 위한 라이브러리로 이산적 형태로 데이터를 바꾸기 위해 선언\n",
    "import numpy as np \n",
    "import time, math, random\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# import gym \n",
    "import gym\n",
    "import pygame                                       #기존 코드에는 없으나 gym의 버전을 맞추기 위해 사용\n",
    "import sys                                          #기존 코드에는 없으나 너무 느린 학습으로 인해 개선시켜보기 위한 라이브러리\n",
    "import multiprocessing                              #기존 코드에는 없으나 너무 느린 학습으로 인해 개선시켜보기 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')                       \n",
    "#이 코드는 gym의 Open AI library에서 CartPole version1을 실행시키기 위한 환경을 gym.make('CartPole-v1')을 통해 불러와 호출하고 이를 env라는 변수에 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple policy function \n",
    "policy = lambda _,__,___, tip_velocity : int( tip_velocity > 0 ) #-,--,---은 해당 위치의 인자 무시\n",
    "#tip_velocity는 끝점의 속도\n",
    "\n",
    "#policy는 막대의 움직임에 따라 카트가 action을 취할 지침을 의미한다.\n",
    "#lambda 함수로 정의되었는데 이 lambda함수는 함수를 간단하게 쓰기 위한 코드이다. \n",
    "#ex) sum = lambda a,b : a+b 이것은 a와b라는 변수의 합을 sum에 대입하라. 라는 뜻이다. 이것을 여기에 적용해보면 \n",
    "#끝점의 속도가 양수이면 true이므로 정수형으로 바꾸어 1을 policy에 대입하고, 끝점의 속도가 음수이면 false이므로 정수형으로 바구어 0을 policy에 대입하라는 뜻이다.\n",
    "#즉, 오른쪽으로 막대가 움직이면 1, 왼쪽으로 막대가 움직이면 0을 policy에 대입하여 향후 Pole이 action을 취할 때 어디로 어떻게 해야하는 지에 영향을 준다.\n",
    "#_,__,___은 해당 위치의 인자들을 무시하기 위해 작성된 것이다. 왜냐하면 인자로 들어오는 currentstate는 discretizer 함수의 반환값인 angle, velocity 등 여러 정보를 가지고 오기 때문이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = ( 6 , 12 )                                                                 #각도, 각속도에 대한 구간의 개수, 연속적인 값인 각도와 각속도를 6개와 12개로 분할하여 연산 및 판단\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]                  #관측 가능한 공간의 최소값 설정\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]                  #관측 가능한 공간의 최대값 설정\n",
    "\n",
    "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:                 #연속적인 상태를 이산적으로 변환\n",
    "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')     #이산 상태를 정수로 인코딩\n",
    "    est.fit([lower_bounds, upper_bounds ])                                          #객체를 훈련하기 위한 범위를 입력\n",
    "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))               #각도와 각속도가 연속 상태의 값이므로 [angle, pole_velocity]를 구간화하여 반환한다. map int를 사용하여 정수로 변환하고 튜플로 바꿔 반환한다.\n",
    "\n",
    "#이산적인 상태 : 연속되지 않고 떨어져있는 상태로 정수와 같이 1,2,3으로 연속되지 않은 상태를 말한다.\n",
    "#인코딩 : 정보나 데이터를 특정 방식으로 표현하거나 변환하는 것으로 여기서는 연속적인 각도와 각속도 값을 정수로 바꾸는 것을 의미한다.\n",
    "\n",
    "#est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')는\n",
    "#주어진 구간 수에 따라 이산 상태를 정수로 인코딩하는 데 사용되는 KBinsDiscretizer 객체를 생성하는 코드이다.\n",
    "#KBinsDiscretizer는 scikit-learn 라이브러리의 클래스로, 연속적인 값을 주어진 구간(bin) 수에 따라 이산적인 값으로 변환한다.\n",
    "\n",
    "#이산화(discretization)는 연속적인 상태를 이산적인 상태로 변환하여 각 상태를 구분할 수 있도록 한다.\n",
    "\n",
    "#n_bins는 구간의 개수를 나타내는 튜플이다. 첫 번째 값은 각도에 대한 구간 수를, 두 번째 값은 각속도에 대한 구간 수를 나타낸다.\n",
    "#ex) 각도가 360라고 한다면 6개의 구간으로 나눠 0도부터 60도는 0, 60도 부터 120도는 1, 120도 부터 180도는 2, ...으로 나타낼 것을 의미한다. 여기서는 -50radian~50radian을 나눈거려나\n",
    "#encode='ordinal'은 이산 상태를 정수로 인코딩하는 방법을 지정하는 매개변수이다. 'ordinal'로 설정하면 각 이산 상태를 0부터 순서대로 정수로 인코딩한다.\n",
    "#strategy='uniform'은 구간을 균등하게 분할하는 방법을 지정하는 매개변수이다. 'uniform'으로 설정하면 구간의 크기를 가능한 한 동일하게 나눈다.\n",
    "\n",
    "#est.fit([lower_bounds, upper_bounds])는 KBinsDiscretizer 객체를 주어진 하한값과 상한값의 범위로 훈련시키는 과정이다. \n",
    "#이를 통해 이산화 객체가 주어진 범위 내에서 이산 상태를 정확하게 구분할 수 있도록 학습된다.\n",
    "\n",
    "#이후 est.transform([[angle, pole_velocity]])[0]를 사용하여 주어진 연속 상태인 각도와 각속도를 이산화된 상태로 변환한다. \n",
    "#변환된 상태는 정수값으로 인코딩되어 있고, tuple(map(int, ...))를 통해 변환된 상태를 정수 튜플로 반환한다. \n",
    "# 이때, 첫번째 구간만을 선택하기 위해 [0]을 사용한 것 같은데 그 이유는 잘 모르겠지만 원하고자 하는 값을 호출한 것이기에 사용자가 지정하는 것 같다.\n",
    "\n",
    "#따라서 est 객체는 연속 상태를 이산 상태로 변환하고, 변환된 상태를 정수로 인코딩하는 데 사용되는 객체이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12, 2)\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape\n",
    "# n_bins는 이산 상태의 개수\n",
    "#env.action_space.n은 이 환경에서 가능한 동작의 개수\n",
    "#이산 상태의 개수와 동작 가능한 개수를 표현할 수 있는 3차원의 Q_table 배열을 만드는데 (6,12,2)이다.\n",
    "#각도와 각속도를 n_bins 크기로 변환하였으니 이 값을 저장하는 차원에 동작을 나타내는 차원을 더해야하니 모든 동작 가능한 개수를 넣어 0으로 미리 초기화시킨다.\n",
    "#[[[0. 0.]\n",
    "#  [0. 0.]\n",
    "#  [0. 0.]\n",
    "#  .\n",
    "#  .\n",
    "#  .\n",
    "#  ]\n",
    "#  [[0. 0.]\n",
    "#  [0. 0. ]\n",
    "#  .\n",
    "#  .\n",
    "# ]             6 x 12 x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy( state : tuple ):\n",
    "    \"\"\"Choosing action based on epsilon-greedy policy\"\"\"\n",
    "    return np.argmax(Q_table[state])\n",
    "#현재 Q_table에서 가장 가치가 높은 것(행동)을 반환한다.\n",
    "#위에서 언급한 바와 같이 policy는 어떻게 움직일 지에 대한 행동지침이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    \"\"\"Temperal diffrence for updating Q-value of state-action pair\"\"\"\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    #Q_table의 새로운 상태에서 가장 가치가 높은 값을 최적의 value로 지정한다.\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    \n",
    "    #reward란 강화학습에서 사용되는 방식인데 지금 카트가 행동을 잘하고 있다고 판단하면 양수의 값을, 막대를 쓰러뜨리는 등 잘못된 행동을 하고 있으면\n",
    "    #음수의 값을 주어 값을 감소시킨다. 그래서 결국 가장 크게 Q_Value를 가지도록 현재를 유지시켜야 한다. 그게 옳은 행동일테니까\n",
    "    #이것은 이 새로운 Q_value를 판단하기 위한 함수이며 env.step()에서 보상, 새로운 상태, discount_factor를 받는다.\n",
    "    #그렇다면 지금 이 행동이 잘한건지 못한건지에 대한 reward에 이 다음 카트의 행동 중 가장 가치가 높다고 판단되는 future_optimal_value에\n",
    "    #discount_factor를 곱하여 learned_value 변수에 저장하여 새로운 Q-value로 learned_value값을 반환한다.(value값을 학습하고 초기화 하는 단계이므로)\n",
    "     \n",
    "    #discount_factor란 Discount factor는 일반적으로 0과 1 사이의 값으로 설정된다. \n",
    "    #값이 1에 가까울수록 미래의 보상을 현재의 보상과 동일한 가치로 간주하고, 값이 0에 가까울수록 미래의 보상을 현재의 보상보다 더 적게 판단한다.\n",
    "    #강화 학습에서의 목표는 미래에 얻을 수 있는 보상을 최대화하는 것이기 때문에, \n",
    "    # discount_factor는 미래 보상을 현재 보상에 비교하여 얼마나 가치있게 여길지를 결정한다.\n",
    "    \n",
    "    #learned_value를 저렇게 구하는 이유는 Markov Decision Process에 기반하는데, 여기서는 ValueFunction을 결정하는 공식이 저것이기 때문이다.\n",
    "    #Value function\n",
    "    #목표로 하는 것 주변에는 value값이 비교적 높고, terminal값(감점을 받는 구간)주변에는 value값이 비교적 낮다.\n",
    "    #value를 계산하기 위해 시뮬레이션 해보자\n",
    "    #value값이 처음에는 0이지만 계속해서 시행착오를 거치며 \n",
    "    #그 지역의 value값을 업데이트 한다. 실패하면 그 행동의 value를 깎고, 성공하면 그 행동의 value를 올리는 방식이다.\n",
    "    \n",
    "# Markov Decision Process is a tuple (S,A,{Psa},r,R)\n",
    "# S is a set of states\n",
    "# A is a set of actions\n",
    "\n",
    "# Psa are the state transition probabilities(s라는 statement에서 a라는 action을 취할 확률\n",
    "# ex 1에서 2로 가는 확률 = P12)\n",
    "\n",
    "# r(gamma) is subset of {0,1} is called the discount factor\n",
    "\n",
    "# R : S X A is the reward function. R가 큰 값으로 움직이기 때문에 그걸 고려해서\n",
    "# reward를 줘야한다.\n",
    "\n",
    "#Thus, value = sum(reward * gamma)\n",
    "#여기서 gamma가 바로 discount_factor이다.\n",
    "    return learned_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive learning of Learning Rate\n",
    "def learning_rate(n : int , min_rate=0.01 ) -> float  : #float값으로 반환\n",
    "    \"\"\"Decaying learning rate\"\"\"\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
    "#개념\n",
    "#최적화된 policy를 위해 초기에는 크게크게 업데이트하여 학습을 시키고 점진적으로 LearningRate의 값의 영향을 낮춰 학습보다는 활용을 강화하는 것이 목적이다.\n",
    "#즉, value값에서 과거의 데이터와 현재의 데이터를 어느 비율로 반영할 것인지에 대한 것이다.\n",
    "#만일 LearningRate의 값의 영향을 낮추지 않는다면 이전의 학습이 의미가 없어지게 된다. \n",
    "# 메인 코드에서 나오겠지만 학습시킬 때, (1-lr)old_value + lr*current_value 공식으로 학습시키게 되는데 lr이 초기에는 크게 하여 과거 경험보단 현재 경험을 중요시하고\n",
    "#학습횟수가 증가할 수록 1-lr이 커지기 때문에 과거 경험을 더 크게 작용하도록 하여 현재 상황에 적용하는 원리이다.\n",
    "\n",
    "#원리\n",
    "#n+1을 25로 나눈 값은 반복될수록 값이 커지게 되며, 이 값이 로그함수에 들어가는 순간,\n",
    "#n이 커질수록 log10((n+1)/25)의 상승률이 적어지게 된다. 이 장치가 바로 학습률을 느리게 감소시키는 장치이다.\n",
    "#이 값을 1에서 빼게 되므로 학습률은 감소하게 되는데 이때, min함수에서 1.0과 비교되어 빠져나간다.\n",
    "#1.0은 학습률에 제한을 걸어둔 것이다. 즉, 학습률은 1.0이상으로 커질 수 없다는 뜻이다.\n",
    "#이때, 학습률이 너무 작아지면 제대로 된 학습이 이루어지지 않기 때문에 max함수를 활용하여\n",
    "#최소 학습률 min_rate보다 적게 학습률이 떨어지지 않게 한다.\n",
    "\n",
    "#적용\n",
    "#여기서 n은 학습횟수이다. 100번 반복시키기에 100까지 n값은 늘어나게 될 것이다.\n",
    "#초기 시행횟수 25번 이하에서의 n들은 25로 나누었을 때 1보다 작거나 같게 되므로 1.0과 비교했을 때 1.0보다 크거나 같다.(ex) log10(16/25 == 0.64) < 0, 1.0 - log10(16/25) < 1.0\n",
    "#때문에 min(1.0,)함수에서 초기 시행단계에서의 최대 학습률을 지정한 것이다. (메인함수에서 old_value의 값이 0이 되도록)\n",
    "#그 이후부터는 1.0보다 작게 되므로 1.0-math.log10((n+1)/25)의 값이 min에서 선택되게 되고 이들은 min_rate == 0.01와 max함수로 비교된다. 최소학습률을 0.01로 지정하였지만 \n",
    "#어차피 주어진 코드에서는 100번만 시행하므로 0.01보다 값이 크게 되므로 min함수에서 나온 값이 선택되어 learning_rate가 되는 것이다. 이때, 반환은 float값으로 반환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
    "    \"\"\"Decaying exploration rate\"\"\"\n",
    "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))\n",
    "#개념\n",
    "#learningRate와 코드가 같지만, 쓰임새는 다르다.\n",
    "#learningRate가 학습률을 나타내어 value값을 변화시킬 때 과거 데이터와 현재 데이터의 조화를 담당했다면,\n",
    "#explorationRate는 탐험률이다. 즉, 어느 정도의 비율로 새로운 행동을 시도해볼 것인가에 대한 여부이다.\n",
    "#Pole이라는 막대기가 움직이는 것은 random한 값을 생성하여 그 값이 이 탐험률보다 작다면 cart가 새로운 행동을 취하는 것이기 때문에 이 탐험률 역시 점점 작아져야 한다.\n",
    "#탐험률이 점점 작아지지 않는다면 이미 프로그램은 최적의 cart 움직임을 완성했는데 굳이 새로운 행동을 시도하는 꼴이 될 수 있다.\n",
    "\n",
    "#원리와 적용은 learningRate와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#함수들 주석 달기\n",
    "#발표 대본\n",
    "\n",
    "n_episodes = 100\n",
    "for e in range(n_episodes):\n",
    "    # Siscretize state into buckets\n",
    "    current_state, done = discretizer(*env.reset()), False\n",
    "    while done==False:\n",
    "        \n",
    "        # policy action \n",
    "        action = policy(current_state) # exploit\n",
    "        \n",
    "        # insert random action\n",
    "        if np.random.random() < exploration_rate(e) : \n",
    "            action = env.action_space.sample() # explore \n",
    "         \n",
    "        # increment enviroment\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretizer(*obs)\n",
    "        \n",
    "        # Update Q-Table\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward , new_state )\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "        \n",
    "        current_state = new_state\n",
    "        \n",
    "\n",
    "        # Render the cartpole environment\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())    \n",
    "        display.clear_output(wait=True)\n",
    "        env.render()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type==pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
